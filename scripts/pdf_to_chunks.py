#!/usr/bin/env python3
"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  üìê Riy_Muellim_Agent ‚Äî PDF D…ôrslik Emal Pipeline
  
  Bu skript PDF d…ôrslikl…ôrd…ôn m…ôtn √ßƒ±xarƒ±r, m√∂vzu-m√∂vzu
  par√ßalayƒ±r v…ô Claude Code-un istifad…ô ed…ôc…ôyi formata
  √ßevirir.
  
  ƒ∞stifad…ô: python3 scripts/pdf_to_chunks.py
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import os
import re
import json
import sys

try:
    import pdfplumber
except ImportError:
    print("‚ùå pdfplumber qura≈üdƒ±rƒ±lmayƒ±b. Qura≈üdƒ±rƒ±n:")
    print("   pip3 install pdfplumber")
    sys.exit(1)

# ‚îÄ‚îÄ‚îÄ KONFƒ∞QURASƒ∞YA ‚îÄ‚îÄ‚îÄ
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PDF_DIR = os.path.join(BASE_DIR, "derslikler", "pdf")
CHUNKS_DIR = os.path.join(BASE_DIR, "derslikler", "chunks")
INDEX_FILE = os.path.join(BASE_DIR, "derslikler", "derslik_index.json")

# Papkalarƒ± yarat
os.makedirs(CHUNKS_DIR, exist_ok=True)

# ‚îÄ‚îÄ‚îÄ B√ñLM∆è/M√ñVZU AYIRICI PATTERN-L∆èR ‚îÄ‚îÄ‚îÄ
# D…ôrslikl…ôrd…ô b√∂lm…ô ba≈ülƒ±qlarƒ± ad…ôt…ôn bel…ô olur:
BOLME_PATTERNS = [
    r'(?:^|\n)\s*(\d+[\-\.]\s*(?:ci|c√º|cƒ±|cu)\s+B√ñLM∆è[.:]\s*.+)',
    r'(?:^|\n)\s*(\d+[\-\.]\s*(?:B√∂lm…ô|B√ñLM∆è)\s*[.:]\s*.+)',
    r'(?:^|\n)\s*((?:I|II|III|IV|V|VI|VII|VIII|IX|X)+\s+B√ñLM∆è[.:]\s*.+)',
    r'(?:^|\n)\s*(\d+\.\s+[A-Z∆è√ú√ñ≈û√áƒûIƒ∞][A-Z∆è√ú√ñ≈û√áƒûIƒ∞\s]{5,})',  # ALL CAPS ba≈ülƒ±q
]

MOVZU_PATTERNS = [
    r'(?:^|\n)\s*(\d+\.\d+[\.\s]+[A-Z∆è√ú√ñ≈û√áƒûIƒ∞a-z…ô√º√∂≈ü√ßƒüƒ±i].{10,})',
    r'(?:^|\n)\s*(M√∂vzu\s*\d*[.:]\s*.+)',
    r'(?:^|\n)\s*(D…ôrs\s*\d*[.:]\s*.+)',
]


def extract_text_from_pdf(pdf_path):
    """PDF-d…ôn b√ºt√ºn m…ôtni √ßƒ±xar, s…ôhif…ô n√∂mr…ôl…ôri il…ô"""
    pages = []
    print(f"  üìñ Oxunur: {os.path.basename(pdf_path)}")
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            total = len(pdf.pages)
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text and len(text.strip()) > 20:  # Bo≈ü/√ßox qƒ±sa s…ôhif…ôl…ôri atla
                    pages.append({
                        "page_num": i + 1,
                        "text": text.strip()
                    })
                
                # Progress
                if (i + 1) % 20 == 0:
                    print(f"    ... {i+1}/{total} s…ôhif…ô")
            
            print(f"    ‚úÖ {len(pages)}/{total} s…ôhif…ô oxundu")
    except Exception as e:
        print(f"    ‚ùå X…ôta: {e}")
        return []
    
    return pages


def detect_chapters(full_text):
    """M…ôtnd…ô b√∂lm…ô/f…ôsil ba≈ülƒ±qlarƒ±nƒ± tap"""
    chapters = []
    
    for pattern in BOLME_PATTERNS:
        matches = re.finditer(pattern, full_text, re.MULTILINE)
        for m in matches:
            title = m.group(1).strip()
            pos = m.start()
            chapters.append({"title": title, "pos": pos, "type": "bolme"})
    
    for pattern in MOVZU_PATTERNS:
        matches = re.finditer(pattern, full_text, re.MULTILINE)
        for m in matches:
            title = m.group(1).strip()
            pos = m.start()
            chapters.append({"title": title, "pos": pos, "type": "movzu"})
    
    # Pozisiyaya g√∂r…ô sƒ±rala
    chapters.sort(key=lambda x: x["pos"])
    
    return chapters


def smart_chunk_pages(pages, grade, part=""):
    """S…ôhif…ôl…ôri aƒüƒ±llƒ± ≈ü…ôkild…ô par√ßala ‚Äî h…ôr 5-10 s…ôhif…ô bir chunk"""
    chunks = []
    
    if not pages:
        return chunks
    
    # B√ºt√ºn m…ôtni birl…ô≈üdir (b√∂lm…ô a≈ükarlamasƒ± √º√ß√ºn)
    full_text = "\n\n".join([f"[S∆èH.{p['page_num']}]\n{p['text']}" for p in pages])
    
    # B√∂lm…ô ba≈ülƒ±qlarƒ±nƒ± tap
    detected = detect_chapters(full_text)
    
    if detected and len(detected) >= 3:
        # A≈ükar edilmi≈ü b√∂lm…ôl…ôr…ô g√∂r…ô par√ßala
        print(f"    üìë {len(detected)} b√∂lm…ô/m√∂vzu tapƒ±ldƒ±")
        
        for i, ch in enumerate(detected):
            start_pos = ch["pos"]
            end_pos = detected[i+1]["pos"] if i+1 < len(detected) else len(full_text)
            
            chunk_text = full_text[start_pos:end_pos].strip()
            
            if len(chunk_text) > 100:  # √áox qƒ±sa olanlarƒ± atla
                # S…ôhif…ô n√∂mr…ôl…ôrini √ßƒ±xar
                page_nums = re.findall(r'\[S∆èH\.(\d+)\]', chunk_text)
                page_range = f"{page_nums[0]}-{page_nums[-1]}" if page_nums else "?"
                
                # T…ômiz ba≈ülƒ±q
                title = ch["title"]
                title = re.sub(r'[^\w\s\-\.]', '', title).strip()[:80]
                
                chunks.append({
                    "grade": grade,
                    "part": part,
                    "title": title,
                    "type": ch["type"],
                    "pages": page_range,
                    "text": re.sub(r'\[S∆èH\.\d+\]\n?', '', chunk_text).strip()
                })
    else:
        # B√∂lm…ô tapƒ±lmadƒ±sa ‚Äî h…ôr 8 s…ôhif…ôni bir chunk et
        print(f"    üìë Avtomatik par√ßalama (h…ôr 8 s…ôhif…ô)")
        
        chunk_size = 8
        for i in range(0, len(pages), chunk_size):
            batch = pages[i:i+chunk_size]
            page_start = batch[0]["page_num"]
            page_end = batch[-1]["page_num"]
            text = "\n\n".join([p["text"] for p in batch])
            
            # ƒ∞lk s…ôtird…ôn ba≈ülƒ±q √ßƒ±xar
            first_line = text.split('\n')[0][:60].strip()
            
            chunks.append({
                "grade": grade,
                "part": part,
                "title": first_line or f"S…ôhif…ô {page_start}-{page_end}",
                "type": "auto",
                "pages": f"{page_start}-{page_end}",
                "text": text
            })
    
    return chunks


def save_chunk(chunk, chunk_num, grade, part):
    """Chunk-ƒ± markdown faylƒ± kimi saxla"""
    
    # Fayl adƒ± √º√ß√ºn t…ômiz ba≈ülƒ±q
    safe_title = chunk["title"].lower()
    safe_title = re.sub(r'[…ô∆è]', 'e', safe_title)
    safe_title = re.sub(r'[√º√ú]', 'u', safe_title)
    safe_title = re.sub(r'[√∂√ñ]', 'o', safe_title)
    safe_title = re.sub(r'[≈ü≈û]', 's', safe_title)
    safe_title = re.sub(r'[√ß√á]', 'c', safe_title)
    safe_title = re.sub(r'[ƒüƒû]', 'g', safe_title)
    safe_title = re.sub(r'[ƒ±ƒ∞]', 'i', safe_title)
    safe_title = re.sub(r'[^\w\s\-]', '', safe_title)
    safe_title = re.sub(r'\s+', '_', safe_title.strip())[:50]
    
    part_str = f"_{part}" if part else ""
    filename = f"sinif{grade}{part_str}_chunk{chunk_num:02d}_{safe_title}.md"
    filepath = os.path.join(CHUNKS_DIR, filename)
    
    content = f"""---
sinif: {grade}
hisse: {part or 'tam'}
movzu: {chunk['title']}
sehifeler: {chunk['pages']}
tip: {chunk['type']}
---

# Sinif {grade} ‚Äî {chunk['title']}
**S…ôhif…ôl…ôr: {chunk['pages']}**

{chunk['text']}
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return filename


def parse_filename(filename):
    """Fayl adƒ±ndan sinif v…ô hiss…ô m…ôlumatƒ±nƒ± √ßƒ±xar"""
    name = filename.lower().replace('.pdf', '')
    
    grade = None
    part = ""
    
    # Sinif n√∂mr…ôsini tap
    # riyaziyyat_6_I_hisse, riyaziyyat_7, etc.
    grade_match = re.search(r'(\d{1,2})', name)
    if grade_match:
        grade = int(grade_match.group(1))
    
    # Hiss…ôni tap
    if 'i_hisse' in name or '1_hisse' in name or 'i hisse' in name:
        if 'ii_hisse' in name or '2_hisse' in name or 'ii hisse' in name:
            part = "II"
        else:
            part = "I"
    elif 'ii_hisse' in name or '2_hisse' in name or 'ii hisse' in name:
        part = "II"
    
    return grade, part


def process_all_pdfs():
    """B√ºt√ºn PDF-l…ôri emal et"""
    
    print("‚ïê" * 60)
    print("  üìê Riy_Muellim_Agent ‚Äî PDF Emal Pipeline")
    print("‚ïê" * 60)
    print()
    
    # PDF fayllarƒ±nƒ± tap
    if not os.path.exists(PDF_DIR):
        print(f"‚ùå PDF papkasƒ± tapƒ±lmadƒ±: {PDF_DIR}")
        print(f"   D…ôrslik PDF-l…ôrini bura qoyun: derslikler/pdf/")
        return
    
    pdf_files = sorted([f for f in os.listdir(PDF_DIR) if f.endswith('.pdf')])
    
    if not pdf_files:
        print(f"‚ùå PDF fayl tapƒ±lmadƒ±: {PDF_DIR}")
        print(f"   D…ôrslik PDF-l…ôrini bura qoyun: derslikler/pdf/")
        return
    
    print(f"üìö {len(pdf_files)} PDF tapƒ±ldƒ±:")
    for f in pdf_files:
        print(f"   ‚Ä¢ {f}")
    print()
    
    # ƒ∞ndeks
    index = {
        "total_pdfs": len(pdf_files),
        "total_chunks": 0,
        "books": []
    }
    
    all_chunks_count = 0
    
    for pdf_file in pdf_files:
        pdf_path = os.path.join(PDF_DIR, pdf_file)
        grade, part = parse_filename(pdf_file)
        
        if grade is None:
            print(f"‚ö†Ô∏è  Sinif m√º…ôyy…ôn edilm…ôdi: {pdf_file}, atlanƒ±r...")
            continue
        
        print(f"\n{'‚îÄ'*60}")
        print(f"  üìó Sinif {grade} {'(' + part + ' hiss…ô)' if part else ''}")
        print(f"{'‚îÄ'*60}")
        
        # 1. M…ôtn √ßƒ±xar
        pages = extract_text_from_pdf(pdf_path)
        
        if not pages:
            print(f"    ‚ö†Ô∏è  M…ôtn √ßƒ±xarƒ±la bilm…ôdi (skan edilmi≈ü PDF ola bil…ôr)")
            continue
        
        # 2. Par√ßala
        chunks = smart_chunk_pages(pages, grade, part)
        
        # 3. Saxla
        book_chunks = []
        for i, chunk in enumerate(chunks, 1):
            filename = save_chunk(chunk, i, grade, part)
            book_chunks.append({
                "file": filename,
                "title": chunk["title"],
                "pages": chunk["pages"],
                "type": chunk["type"],
                "text_length": len(chunk["text"])
            })
            all_chunks_count += 1
        
        print(f"    üíæ {len(chunks)} chunk saxlandƒ± ‚Üí derslikler/chunks/")
        
        # ƒ∞ndeks…ô …ôlav…ô et
        index["books"].append({
            "pdf": pdf_file,
            "grade": grade,
            "part": part,
            "total_pages": len(pages),
            "total_chunks": len(chunks),
            "chunks": book_chunks
        })
    
    # ƒ∞ndeksi saxla
    index["total_chunks"] = all_chunks_count
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'‚ïê'*60}")
    print(f"  ‚úÖ TAMAMLANDI!")
    print(f"  üìö {len(pdf_files)} PDF emal edildi")
    print(f"  üìë {all_chunks_count} chunk yaradƒ±ldƒ±")
    print(f"  üìã ƒ∞ndeks: derslikler/derslik_index.json")
    print(f"{'‚ïê'*60}")
    
    # X√ºlas…ô c…ôdv…ôli
    print(f"\n  ‚îå{'‚îÄ'*8}‚î¨{'‚îÄ'*12}‚î¨{'‚îÄ'*10}‚î¨{'‚îÄ'*10}‚îê")
    print(f"  ‚îÇ{'Sinif':^8}‚îÇ{'Hiss…ô':^12}‚îÇ{'S…ôhif…ô':^10}‚îÇ{'Chunk':^10}‚îÇ")
    print(f"  ‚îú{'‚îÄ'*8}‚îº{'‚îÄ'*12}‚îº{'‚îÄ'*10}‚îº{'‚îÄ'*10}‚î§")
    for book in index["books"]:
        p = book['part'] if book['part'] else 'tam'
        print(f"  ‚îÇ{book['grade']:^8}‚îÇ{p:^12}‚îÇ{book['total_pages']:^10}‚îÇ{book['total_chunks']:^10}‚îÇ")
    print(f"  ‚îî{'‚îÄ'*8}‚î¥{'‚îÄ'*12}‚î¥{'‚îÄ'*10}‚î¥{'‚îÄ'*10}‚îò")


# ‚îÄ‚îÄ‚îÄ ∆èLAV∆è: Chunk Axtarƒ±≈üƒ± (Claude Code istifad…ô ed…ôc…ôk) ‚îÄ‚îÄ‚îÄ

def search_chunks(grade, keyword):
    """M√º…ôyy…ôn sinif v…ô a√ßar s√∂z √ºzr…ô chunk-larƒ± axtar"""
    results = []
    
    if not os.path.exists(CHUNKS_DIR):
        return results
    
    keyword_lower = keyword.lower()
    
    for filename in sorted(os.listdir(CHUNKS_DIR)):
        if not filename.endswith('.md'):
            continue
        
        # Sinif filteri
        if f"sinif{grade}" not in filename:
            continue
        
        filepath = os.path.join(CHUNKS_DIR, filename)
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # A√ßar s√∂z axtarƒ±≈üƒ±
        if keyword_lower in content.lower():
            # ƒ∞lk 500 simvol
            results.append({
                "file": filename,
                "preview": content[:500],
                "full_path": filepath
            })
    
    return results


def search_and_print(grade, keyword):
    """Axtarƒ±≈ü n…ôtic…ôl…ôrini g√∂st…ôr"""
    results = search_chunks(grade, keyword)
    
    if not results:
        print(f"‚ùå Tapƒ±lmadƒ±: sinif {grade}, '{keyword}'")
        return
    
    print(f"\nüîç Tapƒ±ldƒ±: {len(results)} chunk (sinif {grade}, '{keyword}')")
    print("‚îÄ" * 60)
    
    for r in results:
        print(f"\nüìÑ {r['file']}")
        print(r['preview'][:300] + "...")
        print()


# ‚îÄ‚îÄ‚îÄ MAIN ‚îÄ‚îÄ‚îÄ

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "search" and len(sys.argv) >= 4:
            # python3 scripts/pdf_to_chunks.py search 6 faiz
            grade = int(sys.argv[2])
            keyword = " ".join(sys.argv[3:])
            search_and_print(grade, keyword)
        else:
            print("ƒ∞stifad…ô:")
            print("  python3 scripts/pdf_to_chunks.py          # B√ºt√ºn PDF-l…ôri emal et")
            print("  python3 scripts/pdf_to_chunks.py search 6 faiz  # Chunk-larda axtar")
    else:
        process_all_pdfs()
