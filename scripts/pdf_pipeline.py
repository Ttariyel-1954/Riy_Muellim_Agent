#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“ Riy_Muellim_Agent â€” PDF DÉ™rslik Emal Pipeline              â•‘
â•‘                                                                  â•‘
â•‘  Bu skript riyaziyyat dÉ™rsliklÉ™rini (PDF) oxuyur,                â•‘
â•‘  mÃ¶vzulara gÃ¶rÉ™ parÃ§alayÄ±r vÉ™ axtarÄ±ÅŸ Ã¼Ã§Ã¼n indekslÉ™yir.         â•‘
â•‘                                                                  â•‘
â•‘  Ä°stifadÉ™: python3 scripts/pdf_pipeline.py                      â•‘
â•‘  NÉ™ticÉ™:  derslikler/chunks/ â€” JSON chunk fayllarÄ±              â•‘
â•‘           derslikler/index.json â€” Master indeks                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import json
import re
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict

try:
    import pdfplumber
except ImportError:
    print("âŒ pdfplumber yÃ¼klÉ™nmÉ™yib. YÃ¼klÉ™yin:")
    print("   pip install pdfplumber")
    sys.exit(1)


# â”€â”€â”€ KONFÄ°QURASÄ°YA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_DIR = Path(__file__).resolve().parent.parent
PDF_DIR = BASE_DIR / "derslikler" / "pdf"
CHUNKS_DIR = BASE_DIR / "derslikler" / "chunks"
INDEX_FILE = BASE_DIR / "derslikler" / "index.json"

# DÉ™rslik fayl adlarÄ± â†’ sinif + hissÉ™ mapping
TEXTBOOK_MAP = {
    "riyaziyyat_1_I_hisse.pdf":  {"grade": 1, "part": 1, "parts_total": 2},
    "riyaziyyat_1_II_hisse.pdf": {"grade": 1, "part": 2, "parts_total": 2},
    "riyaziyyat_2_I_hisse.pdf":  {"grade": 2, "part": 1, "parts_total": 2},
    "riyaziyyat_2_II_hisse.pdf": {"grade": 2, "part": 2, "parts_total": 2},
    "riyaziyyat_3_I_hisse.pdf":  {"grade": 3, "part": 1, "parts_total": 2},
    "riyaziyyat_3_II_hisse.pdf": {"grade": 3, "part": 2, "parts_total": 2},
    "riyaziyyat_4_I_hisse.pdf":  {"grade": 4, "part": 1, "parts_total": 2},
    "riyaziyyat_4_II_hisse.pdf": {"grade": 4, "part": 2, "parts_total": 2},
    "riyaziyyat_5_I_hisse.pdf":  {"grade": 5, "part": 1, "parts_total": 2},
    "riyaziyyat_5_II_hisse.pdf": {"grade": 5, "part": 2, "parts_total": 2},
    "riyaziyyat_6_I_hisse.pdf":  {"grade": 6, "part": 1, "parts_total": 2},
    "riyaziyyat_6_II_hisse.pdf": {"grade": 6, "part": 2, "parts_total": 2},
    "riyaziyyat_7.pdf":          {"grade": 7, "part": 1, "parts_total": 1},
    "riyaziyyat_8.pdf":          {"grade": 8, "part": 1, "parts_total": 1},
    "riyaziyyat_9.pdf":          {"grade": 9, "part": 1, "parts_total": 1},
    "riyaziyyat_10.pdf":         {"grade": 10, "part": 1, "parts_total": 1},
    "riyaziyyat_11.pdf":         {"grade": 11, "part": 1, "parts_total": 1},
}

# Riyaziyyat mÃ¶vzu aÃ§ar sÃ¶zlÉ™ri (AzÉ™rbaycan dilindÉ™)
TOPIC_KEYWORDS = {
    "ededler": [
        "natural É™dÉ™d", "tam É™dÉ™d", "rasional É™dÉ™d", "irrasional",
        "hÉ™qiqi É™dÉ™d", "mÃ¼rÉ™kkÉ™b É™dÉ™d", "sadÉ™ É™dÉ™d", "bÃ¶lÃ¼nmÉ™",
        "qalÄ±qlÄ± bÃ¶lmÉ™", "bÃ¶yÃ¼k ortaq bÃ¶l", "kiÃ§ik ortaq bÃ¶l",
        "ÆBOB", "ÆKOB", "É™dÉ™dlÉ™r", "rÉ™qÉ™m", "say", "onluq",
        "kÉ™sr", "dÃ¼zgÃ¼n kÉ™sr", "dÃ¼zgÃ¼n olmayan", "qarÄ±ÅŸÄ±q É™dÉ™d",
        "faiz", "nisbÉ™t", "tÉ™nasÃ¼b", "mÃ¼tÉ™nasib"
    ],
    "cebr": [
        "tÉ™nlik", "bÉ™rabÉ™rsizlik", "ifadÉ™", "dÉ™yiÅŸÉ™n", "É™msal",
        "Ã§oxhÉ™dli", "birhÉ™dli", "vuruqlara ayÄ±rma", "kvadrat tÉ™nlik",
        "diskriminant", "kÃ¶klÉ™r", "Vyet teoremi", "sistem",
        "funksiya", "xÉ™tti funksiya", "kvadrat funksiya",
        "qrafik", "ardÄ±cÄ±llÄ±q", "proqressiya", "arifmetik",
        "hÉ™ndÉ™si proqressiya", "limit", "tÃ¶rÉ™mÉ™", "inteqral",
        "logarifm", "triqonometriya", "sin", "cos", "tg"
    ],
    "hendese": [
        "nÃ¶qtÉ™", "dÃ¼z xÉ™tt", "ÅŸÃ¼a", "parÃ§a", "bucaq",
        "Ã¼Ã§bucaq", "dÃ¶rdbucaqlÄ±", "Ã§evrÉ™", "dairÉ™",
        "perimetr", "sahÉ™", "hÉ™cm", "paralel", "perpendikulyar",
        "simmetriya", "oxÅŸarlÄ±q", "bÉ™rabÉ™rlik", "Pifaqor",
        "teorem", "sinus", "kosinus", "tangens",
        "vektor", "koordinat", "fÉ™za", "prizma", "piramida",
        "silindr", "konus", "kÃ¼rÉ™", "mÃ¼stÉ™vi",
        "trapesiya", "paraleloqram", "romb", "dÃ¼zbucaqlÄ±",
        "kvadrat", "beÅŸbucaqlÄ±", "altÄ±bucaqlÄ±", "Ã§oxbucaqlÄ±"
    ],
    "statistika": [
        "statistika", "verilÉ™n", "diaqram", "qrafik",
        "orta", "median", "moda", "yayÄ±lma",
        "ehtimal", "hadisÉ™", "tÉ™sadÃ¼fi", "eksperiment",
        "kombinatorika", "yerlÉ™ÅŸdirmÉ™", "birlÉ™ÅŸmÉ™",
        "dairÉ™vi diaqram", "sÃ¼tunlu diaqram", "xÉ™tti diaqram",
        "tezlik", "cÉ™dvÉ™l", "histoqram"
    ],
    "olcme": [
        "uzunluq", "kÃ¼tlÉ™", "hÉ™cm", "zaman", "pul",
        "metr", "santimetr", "kilometr", "litr",
        "kiloqram", "qram", "ton", "saat", "dÉ™qiqÉ™",
        "saniyÉ™", "manat", "qÉ™pik", "Ã¶lÃ§mÉ™",
        "temperatur", "dÉ™rÉ™cÉ™"
    ]
}

# FÉ™sil/BÃ¶lmÉ™ baÅŸlÄ±q patternlÉ™ri
CHAPTER_PATTERNS = [
    r'(?:FÆSÄ°L|FÆSIL|FÉ™sil)\s*[\dIVXLCM]+[.:]\s*(.+)',
    r'(?:BÃ–LMÆ|BÃ¶lmÉ™|BOLME)\s*[\dIVXLCM]+[.:]\s*(.+)',
    r'(?:MÃ–VZU|MÃ¶vzu|MOVZU)\s*[\dIVXLCM]+[.:]\s*(.+)',
    r'^(\d+)\.\s+([A-ZÆÃœÃ–ÄIÅÃ‡a-zÉ™Ã¼Ã¶ÄŸÄ±ÅŸÃ§].{5,60})$',
    r'^Â§\s*(\d+)\.\s*(.+)',
    r'^(\d+\.\d+)\.\s*(.+)',
]


# â”€â”€â”€ PDF MÆTN Ã‡IXARMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_text_from_pdf(pdf_path: Path) -> List[Dict]:
    """PDF-dÉ™n sÉ™hifÉ™-sÉ™hifÉ™ mÉ™tn Ã§Ä±xarÄ±r."""
    pages = []
    print(f"  ğŸ“– Oxunur: {pdf_path.name}", end="", flush=True)

    try:
        with pdfplumber.open(pdf_path) as pdf:
            total = len(pdf.pages)
            for i, page in enumerate(pdf.pages):
                text = page.extract_text() or ""
                # CÉ™dvÉ™llÉ™ri dÉ™ Ã§Ä±xar
                tables = page.extract_tables() or []
                table_text = ""
                for table in tables:
                    if table:
                        for row in table:
                            cells = [str(c) if c else "" for c in row]
                            table_text += " | ".join(cells) + "\n"

                pages.append({
                    "page_num": i + 1,
                    "text": text.strip(),
                    "tables": table_text.strip(),
                    "has_tables": len(tables) > 0,
                    "char_count": len(text)
                })

                # Progress
                if (i + 1) % 20 == 0:
                    print(f" [{i+1}/{total}]", end="", flush=True)

        print(f" âœ… {len(pages)} sÉ™hifÉ™")
    except Exception as e:
        print(f" âŒ XÉ™ta: {e}")

    return pages


# â”€â”€â”€ MÃ–VZU DETEKTORU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def detect_chapter_boundary(text: str) -> Optional[str]:
    """MÉ™tndÉ™ fÉ™sil/bÃ¶lmÉ™ baÅŸlÄ±ÄŸÄ± axtarÄ±r."""
    for pattern in CHAPTER_PATTERNS:
        match = re.search(pattern, text, re.MULTILINE)
        if match:
            return match.group(0).strip()
    return None


def detect_content_area(text: str) -> str:
    """MÉ™tnin hansÄ± riyaziyyat sahÉ™sinÉ™ aid olduÄŸunu mÃ¼É™yyÉ™n edir."""
    text_lower = text.lower()
    scores = {}
    for area, keywords in TOPIC_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw.lower() in text_lower)
        scores[area] = score

    if max(scores.values()) == 0:
        return "umumi"

    return max(scores, key=scores.get)


def extract_topic_from_text(text: str) -> str:
    """MÉ™tndÉ™n mÃ¶vzu adÄ±nÄ± Ã§Ä±xarmaÄŸa Ã§alÄ±ÅŸÄ±r."""
    # FÉ™sil/mÃ¶vzu baÅŸlÄ±ÄŸÄ±
    for pattern in CHAPTER_PATTERNS:
        match = re.search(pattern, text, re.MULTILINE)
        if match:
            groups = match.groups()
            # Son qrupu baÅŸlÄ±q kimi gÃ¶tÃ¼r
            return groups[-1].strip()[:100]

    # ÆgÉ™r baÅŸlÄ±q tapÄ±lmadÄ±sa, ilk 100 simvol
    first_line = text.split("\n")[0].strip()
    return first_line[:100] if first_line else "AdsÄ±z bÃ¶lmÉ™"


# â”€â”€â”€ CHUNK YARATMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def create_chunks(pages, grade: int, part: int,
                  filename: str, chunk_size: int = 3) -> List[Dict]:
    """
    SÉ™hifÉ™lÉ™ri mÃ¶vzu É™saslÄ± chunk-lara bÃ¶l.

    Strategiya:
    1. FÉ™sil/bÃ¶lmÉ™ baÅŸlÄ±qlarÄ± tapÄ±ldÄ±qda yeni chunk baÅŸla
    2. Æks halda, hÉ™r chunk_size sÉ™hifÉ™ni birlÉ™ÅŸdir
    3. HÉ™r chunk-a metadata É™lavÉ™ et
    """
    chunks = []
    current_chunk_pages = []
    current_chapter = None
    chunk_id = 0

    for page in pages:
        text = page["text"]
        if not text or len(text) < 30:
            continue

        # FÉ™sil baÅŸlÄ±ÄŸÄ±?
        new_chapter = detect_chapter_boundary(text)
        if new_chapter and current_chunk_pages:
            # ÆvvÉ™lki chunk-Ä± saxla
            chunk_id += 1
            chunks.append(_build_chunk(
                current_chunk_pages, grade, part, filename,
                chunk_id, current_chapter
            ))
            current_chunk_pages = []
            current_chapter = new_chapter

        if new_chapter:
            current_chapter = new_chapter

        current_chunk_pages.append(page)

        # chunk_size sÉ™hifÉ™yÉ™ Ã§atdÄ±qda (fÉ™sil baÅŸlÄ±ÄŸÄ± olmadan)
        if len(current_chunk_pages) >= chunk_size and not new_chapter:
            chunk_id += 1
            chunks.append(_build_chunk(
                current_chunk_pages, grade, part, filename,
                chunk_id, current_chapter
            ))
            current_chunk_pages = []

    # Son qalan sÉ™hifÉ™lÉ™r
    if current_chunk_pages:
        chunk_id += 1
        chunks.append(_build_chunk(
            current_chunk_pages, grade, part, filename,
            chunk_id, current_chapter
        ))

    return chunks


def _build_chunk(pages, grade: int, part: int,
                 filename: str, chunk_id: int, chapter=None) -> dict:
    """Bir chunk obyekti yaradÄ±r."""
    full_text = "\n\n".join(p["text"] for p in pages if p["text"])
    table_text = "\n".join(p["tables"] for p in pages if p["tables"])

    page_start = pages[0]["page_num"]
    page_end = pages[-1]["page_num"]

    content_area = detect_content_area(full_text)
    topic = extract_topic_from_text(full_text) if not chapter else chapter

    # Unikal ID
    uid = hashlib.md5(f"{filename}_{chunk_id}_{page_start}".encode()).hexdigest()[:12]

    return {
        "id": f"g{grade}_p{part}_c{chunk_id:03d}_{uid}",
        "grade": grade,
        "part": part,
        "source_file": filename,
        "page_start": page_start,
        "page_end": page_end,
        "chapter": chapter,
        "topic": topic,
        "content_area": content_area,
        "text": full_text,
        "tables": table_text,
        "char_count": len(full_text),
        "word_count": len(full_text.split()),
        "has_tables": any(p["has_tables"] for p in pages),
        "keywords": _extract_keywords(full_text)
    }


def _extract_keywords(text: str) -> List[str]:
    """MÉ™tndÉ™n aÃ§ar sÃ¶zlÉ™r Ã§Ä±xarÄ±r."""
    keywords = set()
    text_lower = text.lower()
    for area, kws in TOPIC_KEYWORDS.items():
        for kw in kws:
            if kw.lower() in text_lower:
                keywords.add(kw)
    return sorted(keywords)[:20]


# â”€â”€â”€ Ä°NDEKS YARATMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_master_index(all_chunks) -> dict:
    """BÃ¼tÃ¼n chunk-lar Ã¼Ã§Ã¼n master axtarÄ±ÅŸ indeksi yaradÄ±r."""

    # Sinif â†’ mÃ¶vzu â†’ chunk ID mapping
    grade_index = {}
    topic_index = {}
    content_area_index = {}
    keyword_index = {}

    for chunk in all_chunks:
        g = chunk["grade"]
        ca = chunk["content_area"]
        cid = chunk["id"]

        # Sinif indeksi
        grade_index.setdefault(str(g), []).append(cid)

        # MÃ¶vzu indeksi
        if chunk["topic"]:
            topic_key = chunk["topic"][:60].lower()
            topic_index.setdefault(topic_key, []).append(cid)

        # Content area indeksi
        content_area_index.setdefault(ca, []).append(cid)

        # Keyword indeksi
        for kw in chunk["keywords"]:
            keyword_index.setdefault(kw.lower(), []).append(cid)

    return {
        "created_at": datetime.now().isoformat(),
        "total_chunks": len(all_chunks),
        "grades": sorted(set(str(c["grade"]) for c in all_chunks)),
        "content_areas": sorted(set(c["content_area"] for c in all_chunks)),
        "index": {
            "by_grade": grade_index,
            "by_topic": topic_index,
            "by_content_area": content_area_index,
            "by_keyword": keyword_index
        },
        "chunk_summary": [
            {
                "id": c["id"],
                "grade": c["grade"],
                "part": c["part"],
                "pages": f"{c['page_start']}-{c['page_end']}",
                "topic": c["topic"][:80],
                "content_area": c["content_area"],
                "words": c["word_count"]
            }
            for c in all_chunks
        ]
    }


# â”€â”€â”€ ÆSAS PROQRAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ğŸ“ Riy_Muellim_Agent â€” PDF DÉ™rslik Emal Pipeline          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    # PapkalarÄ± yarat
    CHUNKS_DIR.mkdir(parents=True, exist_ok=True)

    # PDF fayllarÄ± tap
    pdf_files = sorted(PDF_DIR.glob("*.pdf"))
    if not pdf_files:
        print(f"âŒ PDF tapÄ±lmadÄ±: {PDF_DIR}")
        print("   DÉ™rsliklÉ™ri bu papkaya qoyun:")
        print(f"   {PDF_DIR}/")
        print()
        print("   GÃ¶zlÉ™nilÉ™n fayllar:")
        for name in sorted(TEXTBOOK_MAP.keys()):
            print(f"     â€¢ {name}")
        sys.exit(1)

    print(f"ğŸ“ PDF papkasÄ±: {PDF_DIR}")
    print(f"ğŸ“ Chunk papkasÄ±: {CHUNKS_DIR}")
    print(f"ğŸ“„ TapÄ±lmÄ±ÅŸ PDF: {len(pdf_files)}")
    print()

    # TapÄ±lmayan fayllarÄ± yoxla
    found = {f.name for f in pdf_files}
    expected = set(TEXTBOOK_MAP.keys())
    missing = expected - found
    extra = found - expected

    if missing:
        print(f"âš ï¸  Ã‡atÄ±ÅŸmayan dÉ™rsliklÉ™r ({len(missing)}):")
        for m in sorted(missing):
            print(f"   â€¢ {m}")
        print()

    if extra:
        print(f"â„¹ï¸  ÆlavÉ™ PDF-lÉ™r (emal edilmÉ™yÉ™cÉ™k): {sorted(extra)}")
        print()

    # â•â•â• HÆR PDF-NÄ° EMAL ET â•â•â•
    all_chunks = []
    stats = {"files": 0, "pages": 0, "chunks": 0, "words": 0}

    for pdf_file in pdf_files:
        if pdf_file.name not in TEXTBOOK_MAP:
            continue

        meta = TEXTBOOK_MAP[pdf_file.name]
        grade = meta["grade"]
        part = meta["part"]

        print(f"â”â”â” Sinif {grade}, HissÉ™ {part} â”â”â”")

        # 1. MÉ™tn Ã§Ä±xar
        pages = extract_text_from_pdf(pdf_file)
        if not pages:
            continue

        # 2. Chunk-lara bÃ¶l
        chunks = create_chunks(pages, grade, part, pdf_file.name)
        print(f"  ğŸ“¦ {len(chunks)} chunk yaradÄ±ldÄ±")

        # 3. Chunk-larÄ± fayla yaz
        chunk_file = CHUNKS_DIR / f"sinif{grade}_hisse{part}_chunks.json"
        with open(chunk_file, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ SaxlandÄ±: {chunk_file.name}")

        # 4. Tam mÉ™tni dÉ™ saxla (reference Ã¼Ã§Ã¼n)
        full_text_file = CHUNKS_DIR / f"sinif{grade}_hisse{part}_fulltext.txt"
        with open(full_text_file, "w", encoding="utf-8") as f:
            for page in pages:
                f.write(f"\n{'='*60}\n")
                f.write(f"SÆHÄ°FÆ {page['page_num']}\n")
                f.write(f"{'='*60}\n\n")
                f.write(page["text"])
                f.write("\n")
                if page["tables"]:
                    f.write(f"\n[CÆDVÆL]\n{page['tables']}\n")
        print(f"  ğŸ“ Tam mÉ™tn: {full_text_file.name}")

        all_chunks.extend(chunks)
        stats["files"] += 1
        stats["pages"] += len(pages)
        stats["chunks"] += len(chunks)
        stats["words"] += sum(c["word_count"] for c in chunks)
        print()

    # â•â•â• MASTER Ä°NDEKS â•â•â•
    if all_chunks:
        print("â”â”â” Master Ä°ndeks YaradÄ±lÄ±r â”â”â”")
        index = build_master_index(all_chunks)
        with open(INDEX_FILE, "w", encoding="utf-8") as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ“Š SaxlandÄ±: {INDEX_FILE.name}")
        print()

    # â•â•â• NÆTÄ°CÆ STATÄ°STÄ°KA â•â•â•
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ğŸ“Š NÆTÄ°CÆ                                                 â•‘")
    print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print(f"â•‘  ğŸ“„ Emal edilÉ™n PDF:     {stats['files']:>4}                              â•‘")
    print(f"â•‘  ğŸ“ƒ Ãœmumi sÉ™hifÉ™:        {stats['pages']:>4}                              â•‘")
    print(f"â•‘  ğŸ“¦ YaradÄ±lmÄ±ÅŸ chunk:    {stats['chunks']:>4}                              â•‘")
    print(f"â•‘  ğŸ“ Ãœmumi sÃ¶z sayÄ±:     {stats['words']:>6}                            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    # Content area paylanmasÄ±
    ca_counts = {}
    for c in all_chunks:
        ca_counts[c["content_area"]] = ca_counts.get(c["content_area"], 0) + 1
    print("  MÉ™zmun sahÉ™si paylanmasÄ±:")
    for ca, cnt in sorted(ca_counts.items(), key=lambda x: -x[1]):
        bar = "â–ˆ" * (cnt * 30 // max(ca_counts.values()))
        print(f"    {ca:15s} {bar} {cnt}")
    print()

    print("âœ… Pipeline tamamlandÄ±!")
    print()
    print("NÃ¶vbÉ™ti addÄ±m: Claude Code-da bu É™mri iÅŸlÉ™din:")
    print('  "6-cÄ± sinif, FaizlÉ™r, 15 test tapÅŸÄ±rÄ±ÄŸÄ± yaz"')
    print()


if __name__ == "__main__":
    main()
