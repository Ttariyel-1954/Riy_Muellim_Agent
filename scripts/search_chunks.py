#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ” DÉ™rslik Chunk AxtarÄ±ÅŸ Utility                               â•‘
â•‘                                                                  â•‘
â•‘  Sinif + mÃ¶vzu verildikdÉ™ mÃ¼vafiq dÉ™rslik chunk-larÄ±nÄ± tapÄ±r.   â•‘
â•‘                                                                  â•‘
â•‘  Ä°stifadÉ™:                                                       â•‘
â•‘    python3 scripts/search_chunks.py --grade 6 --topic "faiz"    â•‘
â•‘    python3 scripts/search_chunks.py --grade 8 --topic "Pifaqor" â•‘
â•‘    python3 scripts/search_chunks.py --grade 5 --area "ededler"  â•‘
â•‘    python3 scripts/search_chunks.py --grade 3 --list-topics     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import json
import argparse
from pathlib import Path


BASE_DIR = Path(__file__).resolve().parent.parent
CHUNKS_DIR = BASE_DIR / "derslikler" / "chunks"
INDEX_FILE = BASE_DIR / "derslikler" / "index.json"


def load_index() -> dict | None:
    """Master indeksi yÃ¼klÉ™."""
    if not INDEX_FILE.exists():
        print("âŒ Ä°ndeks tapÄ±lmadÄ±. ÆvvÉ™lcÉ™ pipeline iÅŸlÉ™din:")
        print("   python3 scripts/pdf_pipeline.py")
        return None
    with open(INDEX_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def load_chunks_for_grade(grade: int) -> list[dict]:
    """MÃ¼É™yyÉ™n sinif Ã¼Ã§Ã¼n bÃ¼tÃ¼n chunk-larÄ± yÃ¼klÉ™."""
    chunks = []
    for chunk_file in sorted(CHUNKS_DIR.glob(f"sinif{grade}_*_chunks.json")):
        with open(chunk_file, "r", encoding="utf-8") as f:
            chunks.extend(json.load(f))
    return chunks


def search_by_topic(grade: int, topic: str, max_results: int = 5) -> list[dict]:
    """Sinif + mÃ¶vzu Ã¼zrÉ™ axtarÄ±ÅŸ."""
    chunks = load_chunks_for_grade(grade)
    if not chunks:
        return []

    topic_lower = topic.lower()
    topic_words = topic_lower.split()

    scored = []
    for chunk in chunks:
        score = 0
        searchable = (
            chunk["text"].lower() + " " +
            (chunk["topic"] or "").lower() + " " +
            (chunk["chapter"] or "").lower() + " " +
            " ".join(chunk.get("keywords", []))
        )

        # Tam uyÄŸunluq
        if topic_lower in searchable:
            score += 10

        # SÃ¶z-sÃ¶z uyÄŸunluq
        for word in topic_words:
            if len(word) >= 3:  # QÄ±sa sÃ¶zlÉ™ri keÃ§
                count = searchable.count(word)
                score += min(count, 5)  # Max 5 hit per word

        # Keyword uyÄŸunluq
        for kw in chunk.get("keywords", []):
            if any(w in kw.lower() for w in topic_words):
                score += 3

        # Chapter uyÄŸunluq (bÃ¶yÃ¼k bonus)
        if chunk.get("chapter"):
            if topic_lower in chunk["chapter"].lower():
                score += 15

        if score > 0:
            scored.append((score, chunk))

    # Æn yÃ¼ksÉ™k xal sÄ±rasÄ±
    scored.sort(key=lambda x: -x[0])
    return [c for _, c in scored[:max_results]]


def search_by_content_area(grade: int, area: str) -> list[dict]:
    """Sinif + mÉ™zmun sahÉ™si Ã¼zrÉ™ axtarÄ±ÅŸ."""
    chunks = load_chunks_for_grade(grade)
    return [c for c in chunks if c["content_area"] == area]


def search_by_keyword(keyword: str) -> list[dict]:
    """AÃ§ar sÃ¶z Ã¼zrÉ™ bÃ¼tÃ¼n siniflÉ™rdÉ™ axtarÄ±ÅŸ."""
    index = load_index()
    if not index:
        return []

    keyword_lower = keyword.lower()
    chunk_ids = set()

    for kw, ids in index["index"]["by_keyword"].items():
        if keyword_lower in kw:
            chunk_ids.update(ids)

    # Chunk-larÄ± yÃ¼klÉ™
    results = []
    for grade_str in index["grades"]:
        grade = int(grade_str)
        chunks = load_chunks_for_grade(grade)
        for chunk in chunks:
            if chunk["id"] in chunk_ids:
                results.append(chunk)

    return results


def list_topics_for_grade(grade: int) -> list[str]:
    """MÃ¼É™yyÉ™n sinif Ã¼Ã§Ã¼n mÃ¶vcud mÃ¶vzularÄ± siyahÄ±la."""
    chunks = load_chunks_for_grade(grade)
    topics = []
    for chunk in chunks:
        if chunk.get("chapter"):
            topics.append(f"ğŸ“ {chunk['chapter']} (sÉ™h. {chunk['page_start']}-{chunk['page_end']})")
        elif chunk.get("topic"):
            topics.append(f"  â€¢ {chunk['topic'][:80]} (sÉ™h. {chunk['page_start']}-{chunk['page_end']})")
    return topics


def format_chunk_for_output(chunk: dict, include_text: bool = True) -> str:
    """Chunk-Ä± oxunaqlÄ± formatda Ã§ap et."""
    lines = []
    lines.append(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    lines.append(f"â”‚ ğŸ“¦ Chunk: {chunk['id']}")
    lines.append(f"â”‚ ğŸ“š Sinif: {chunk['grade']}, HissÉ™: {chunk['part']}")
    lines.append(f"â”‚ ğŸ“„ SÉ™hifÉ™: {chunk['page_start']}-{chunk['page_end']}")
    lines.append(f"â”‚ ğŸ“ FÉ™sil: {chunk.get('chapter', 'â€”')}")
    lines.append(f"â”‚ ğŸ·ï¸  MÃ¶vzu: {chunk.get('topic', 'â€”')[:80]}")
    lines.append(f"â”‚ ğŸ“Š SahÉ™: {chunk['content_area']}")
    lines.append(f"â”‚ ğŸ“ SÃ¶z: {chunk['word_count']}, Simvol: {chunk['char_count']}")
    lines.append(f"â”‚ ğŸ”‘ AÃ§ar sÃ¶zlÉ™r: {', '.join(chunk.get('keywords', [])[:10])}")
    if chunk.get("has_tables"):
        lines.append(f"â”‚ ğŸ“Š CÉ™dvÉ™l: BÉ™li")
    lines.append(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    if include_text:
        text = chunk["text"]
        if len(text) > 2000:
            text = text[:2000] + "\n... [kÉ™sildi]"
        lines.append("")
        lines.append(text)
        lines.append("")

    return "\n".join(lines)


def get_context_for_generation(grade: int, topic: str) -> str:
    """
    TEST/DÆRS PLANI GENERASIYASI ÃœÃ‡ÃœN KONTEKST HAZIRLA.

    Bu funksiya Claude Code-un É™sas axtarÄ±ÅŸ funksiyasÄ±dÄ±r.
    Sinif + mÃ¶vzu verilir, dÉ™rslikdÉ™n mÃ¼vafiq kontekst qaytarÄ±lÄ±r.
    """
    results = search_by_topic(grade, topic, max_results=3)

    if not results:
        return f"âš ï¸ Sinif {grade}, mÃ¶vzu '{topic}' Ã¼Ã§Ã¼n dÉ™rslik konteksti tapÄ±lmadÄ±."

    output = []
    output.append(f"â•â•â• DÆRSLÄ°K KONTEKSTÄ°: Sinif {grade}, Â«{topic}Â» â•â•â•")
    output.append("")

    for i, chunk in enumerate(results, 1):
        output.append(f"â”â”â” MÉ™nbÉ™ {i}: {chunk.get('source_file', '?')}, "
                       f"sÉ™h. {chunk['page_start']}-{chunk['page_end']} â”â”â”")
        if chunk.get("chapter"):
            output.append(f"FÉ™sil: {chunk['chapter']}")
        output.append("")
        # Tam mÉ™tni ver (max 3000 simvol per chunk)
        text = chunk["text"]
        if len(text) > 3000:
            text = text[:3000] + "\n... [davamÄ± var, sÉ™h. " + str(chunk['page_end']) + "]"
        output.append(text)
        output.append("")

        if chunk.get("tables"):
            output.append("[CÆDVÆLLÆR]")
            output.append(chunk["tables"][:1000])
            output.append("")

    output.append(f"â•â•â• KONTEKSTÄ°N SONU â•â•â•")
    return "\n".join(output)


# â”€â”€â”€ KLÄ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description="ğŸ“ Riyaziyyat dÉ™rslik chunk axtarÄ±ÅŸÄ±"
    )
    parser.add_argument("--grade", "-g", type=int, help="Sinif (1-11)")
    parser.add_argument("--topic", "-t", type=str, help="MÃ¶vzu adÄ±")
    parser.add_argument("--area", "-a", type=str,
                        choices=["ededler", "cebr", "hendese", "statistika", "olcme"],
                        help="MÉ™zmun sahÉ™si")
    parser.add_argument("--keyword", "-k", type=str, help="AÃ§ar sÃ¶z (bÃ¼tÃ¼n siniflÉ™rdÉ™)")
    parser.add_argument("--list-topics", "-l", action="store_true",
                        help="Sinif Ã¼Ã§Ã¼n mÃ¶vzu siyahÄ±sÄ±")
    parser.add_argument("--context", "-c", action="store_true",
                        help="AI generasiya Ã¼Ã§Ã¼n kontekst formatÄ±")
    parser.add_argument("--max", "-m", type=int, default=5,
                        help="Maksimum nÉ™ticÉ™ sayÄ±")
    parser.add_argument("--stats", "-s", action="store_true",
                        help="Ãœmumi statistika gÃ¶stÉ™r")
    parser.add_argument("--no-text", action="store_true",
                        help="MÉ™tn gÃ¶stÉ™rmÉ™, yalnÄ±z metadata")

    args = parser.parse_args()

    # Statistika
    if args.stats:
        index = load_index()
        if index:
            print(f"\nğŸ“Š DÉ™rslik Ä°ndeks StatistikasÄ±")
            print(f"   YaradÄ±lma: {index['created_at']}")
            print(f"   Chunk sayÄ±: {index['total_chunks']}")
            print(f"   Sinifler: {', '.join(index['grades'])}")
            print(f"   SahÉ™lÉ™r: {', '.join(index['content_areas'])}")
            print()
            print("   Sinif Ã¼zrÉ™ chunk sayÄ±:")
            for g, ids in sorted(index["index"]["by_grade"].items(), key=lambda x: int(x[0])):
                bar = "â–ˆ" * (len(ids) * 30 // max(len(v) for v in index["index"]["by_grade"].values()))
                print(f"   Sinif {g:>2}: {bar} {len(ids)}")
        return

    # MÃ¶vzu siyahÄ±sÄ±
    if args.list_topics and args.grade:
        topics = list_topics_for_grade(args.grade)
        if topics:
            print(f"\nğŸ“š Sinif {args.grade} â€” MÃ¶vcud MÃ¶vzular:\n")
            for t in topics:
                print(f"  {t}")
        else:
            print(f"âŒ Sinif {args.grade} Ã¼Ã§Ã¼n chunk tapÄ±lmadÄ±.")
        return

    # AI kontekst
    if args.context and args.grade and args.topic:
        context = get_context_for_generation(args.grade, args.topic)
        print(context)
        return

    # MÃ¶vzu axtarÄ±ÅŸÄ±
    if args.grade and args.topic:
        results = search_by_topic(args.grade, args.topic, args.max)
        if results:
            print(f"\nğŸ” Sinif {args.grade}, mÃ¶vzu Â«{args.topic}Â» â€” {len(results)} nÉ™ticÉ™:\n")
            for chunk in results:
                print(format_chunk_for_output(chunk, include_text=not args.no_text))
        else:
            print(f"âŒ NÉ™ticÉ™ tapÄ±lmadÄ±: sinif {args.grade}, mÃ¶vzu Â«{args.topic}Â»")
        return

    # Content area axtarÄ±ÅŸÄ±
    if args.grade and args.area:
        results = search_by_content_area(args.grade, args.area)
        print(f"\nğŸ” Sinif {args.grade}, sahÉ™ Â«{args.area}Â» â€” {len(results)} chunk:\n")
        for chunk in results[:args.max]:
            print(format_chunk_for_output(chunk, include_text=not args.no_text))
        return

    # Keyword axtarÄ±ÅŸÄ±
    if args.keyword:
        results = search_by_keyword(args.keyword)
        print(f"\nğŸ” AÃ§ar sÃ¶z Â«{args.keyword}Â» â€” {len(results)} nÉ™ticÉ™:\n")
        for chunk in results[:args.max]:
            print(format_chunk_for_output(chunk, include_text=not args.no_text))
        return

    parser.print_help()


if __name__ == "__main__":
    main()
